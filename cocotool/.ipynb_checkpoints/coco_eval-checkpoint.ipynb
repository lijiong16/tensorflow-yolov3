{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngt_anno=[]\\nimg=[]\\nid=0\\nfor root, dirs, files in os.walk(\"../mAP/ground-truth/\"):\\n    for file in files:\\n        lines=ReadTxt(root+file)\\n        for line in lines:\\n            tmp=line.split(\\' \\')[1:]\\n            tmp_anno={\"image_id\":int(file[:-4]),\\n                      \"category_id\":0,\\n                      \"bbox\":list(map(float,tmp)),\\n                     \"id\":id,\\n                     \"iscrowd\":0}\\n            \\n            gt_anno.append(tmp_anno)\\n            img.append({\"id\":id})\\n            id+=1\\nwith open(\"gt_eval.json\", \"w\") as outfile:\\n    json.dump({\"images\":img,\\n               \"annotations\":gt_anno,\\n                \"categories\": [{\"supercategory\": \"person\", \"id\": 0, \"name\": \"person\"}]}, outfile)\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pdb\n",
    "p=pdb.set_trace\n",
    "def ReadTxt(rootdir):\n",
    "    lines = []\n",
    "    with open(rootdir, 'r') as file_to_read:\n",
    "        while True:\n",
    "            line = file_to_read.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            line = line.strip('\\n')\n",
    "            lines.append(line)\n",
    "    return lines\n",
    "\n",
    "\n",
    "pre_anno=[]\n",
    "for root, dirs, files in os.walk(\"../mAP/predicted/\"):\n",
    "    for file in files:\n",
    "        lines=ReadTxt(root+file)\n",
    "        for line in lines:\n",
    "            tmp=line.split(' ')[2:]\n",
    "            bbox=list(map(float,tmp))\n",
    "            tmp_anno={\"image_id\":int(file[:-4]),\n",
    "                      \"category_id\":1,\n",
    "                      \"bbox\":[bbox[0],bbox[1],bbox[2]-bbox[0],bbox[3]-bbox[1]],\n",
    "                      \"score\":float(line.split(' ')[1])}\n",
    "            pre_anno.append(tmp_anno)\n",
    "with open(\"pre_eval.json\", \"w\") as outfile:\n",
    "    json.dump(pre_anno, outfile)\n",
    "\n",
    "\"\"\"\n",
    "gt_anno=[]\n",
    "img=[]\n",
    "id=0\n",
    "for root, dirs, files in os.walk(\"../mAP/ground-truth/\"):\n",
    "    for file in files:\n",
    "        lines=ReadTxt(root+file)\n",
    "        for line in lines:\n",
    "            tmp=line.split(' ')[1:]\n",
    "            tmp_anno={\"image_id\":int(file[:-4]),\n",
    "                      \"category_id\":0,\n",
    "                      \"bbox\":list(map(float,tmp)),\n",
    "                     \"id\":id,\n",
    "                     \"iscrowd\":0}\n",
    "            \n",
    "            gt_anno.append(tmp_anno)\n",
    "            img.append({\"id\":id})\n",
    "            id+=1\n",
    "with open(\"gt_eval.json\", \"w\") as outfile:\n",
    "    json.dump({\"images\":img,\n",
    "               \"annotations\":gt_anno,\n",
    "                \"categories\": [{\"supercategory\": \"person\", \"id\": 0, \"name\": \"person\"}]}, outfile)\n",
    "\"\"\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "annType = ['segm','bbox','keypoints']\n",
    "annType = annType[1]      #specify type here\n",
    "prefix = 'person_keypoints' if annType=='keypoints' else 'instances'\n",
    "#print 'Running demo for *%s* results.'%(annType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.98s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#initialize COCO ground truth api\n",
    "dataDir='../'\n",
    "dataType='val2014'\n",
    "annFile = '%s/annotations/%s_%s.json'%(dataDir,prefix,dataType)\n",
    "annFile=('../data/dataset/instances_val2017.json')\n",
    "cocoGt=COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=1.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#initialize COCO detections api\n",
    "resFile='%s/results/%s_%s_fake%s100_results.json'\n",
    "resFile = resFile%(dataDir, prefix, dataType, annType)\n",
    "resFile=('pre_eval.json')\n",
    "cocoDt=cocoGt.loadRes(resFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgIds=sorted(cocoGt.getImgIds())\n",
    "#imgIds=imgIds[0:100]\n",
    "#imgId = imgIds[np.random.randint(100)]\n",
    "import json \n",
    "dts = json.load(open(resFile,'r'))\n",
    "imgIds = [imid['image_id'] for imid in dts]\n",
    "imgIds = sorted(list(set(imgIds)))\n",
    "del dts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=14.98s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.26s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.422\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.732\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.426\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.176\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.495\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.679\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.438\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.231\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.554\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.742\n"
     ]
    }
   ],
   "source": [
    "# running evaluation\n",
    "cocoEval = COCOeval(cocoGt,cocoDt,annType)\n",
    "cocoEval.params.imgIds  = imgIds\n",
    "cocoEval.params.catIds = [1]\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
